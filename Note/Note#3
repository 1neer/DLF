이번 딥러닝 프레임워크 5주차, 6주차에서는 함수들을 최적화 하는 작업과 고차미분에 대한 이론을 배웠다.

먼저 지금까지의 Dezero는 연산자 구현을 통해 복잡한 함수의 미분도 가능하도록 만들어졌다. 떄문에 5주차에서는 직접
복잡한 함수의 미분을 해보았다.

먼저 Sphere 함수, matyas함수, Goldstein-Price함수 등을 우리가 구현해 둔 연산자를 통해 미분이 가능하였고,
이러한 계산 그래프들을 시각화 할 필요가 있었다. 이유는 복잡한 식을 계산할 때 계산 그래프가 만들어지는 전모를 직접 확인
문제가 발생했을 때 원인이 되는 부분을 파악하기 쉬움
더 나은 계산 방법을 발견할 수 있음
신경망의 구조를 3자에게 시각적으로 전달하는 용도로 활용

이러한 이유등으로 계산 그래프를 시각화 할 필요가 있었는데 이를 위해서 Graphviz를 활용해 계산 그래프를 시각화 하는 과정을 거쳤다.
이것은 Dot 언어를 통해 시각화를 진행하는데, 노드를 만들고, 노드를 연결하면 되는 간단한 언어이다.
그리고 시각화 코드를 통해 Dot언어로 구현한 내용을 화면에 띄워주었다.

그렇다면 우리가 계산한 계산 그래프를 dot언어로 변환하는 과정을 거쳐야 하는데, 이에대한 방법은 
Variable 인스턴스를 건네면 인스턴스 내용을 DOT 언어로 작성된 문자열로 바꿔서 변환하고,
변수 노드에 고유한 ID를 부여하기 위해 파이썬 내장 함수인 id를 사용하며,
id 함수에서 반환하는 객체 ID는 다른 객체와 중복되지 않아서 노드의 ID로 사용하기 적합하기때문에
format 메서드 문자열의 “{ }” 부분을 인수로 건넨 객체로 차례로 바꿔주는 작업을 진행하였다.

다음으로는 테일러급수의 미분에 대해 알아보았다. 먼저 테일러 급수란 테일러 급수는 어떤 미지의 함수를 동일한 미분계수를 갖는 어떤 다항함수로 근사시키는
것이며, sin함수의 미분을 예로 들어서 테일러 급수의 미분을 알아보았다.

또한 함수의 최적화를 진행하였다. 최적화란 어떤 함수가 주어졌을 때 그 최솟값(또는 최댓값)을 반환하는 입력(함수의 인수)
을 찾는 일을 말하는데, 이에대해 로젠브록 함수를 예로들어서 공부하였다.
마지막으로 경사하강법에 대해 배웠다. 경사하강법이란 기울기 방향으로 일정 거리만큼 이동하여 다시 기울기를 구하는 작업을 반복하면 점차 최
솟값(혹은 최댓값)에 접근하리라 기대할 수 있다는 것을 근거로 만들어진 방법이며, 반복횟수를 늘리면 늘릴수록 최솟값에 가까워 지는것을 볼 수 있었지만,
시행횟수가 너무 많다는 단점이 있었다. 때문에 뉴턴방법을 이용한 최적화가 등장하였다. 

뉴턴방법을 적용하면 더 적은 단계로 최적의 결과를 얻을 가능성이 높다.
하지만 아직 dezero에는 고차미분에 대한 구현이 되어있지 않으므로 이를 구현할 필요성이 있었다.
먼저 역전파를 진행할떄 ndarray가 아닌 Variable인스턴스를 사용하여 계산의 연결을 만들었고,
Variable 클래스의 grad가 Variable 인스턴스를 참조하도록 구현하였다.
또한 이러한 변경을 모든 함수에도 적용시킬 필요가 있었다.

이런식으로 고차미분을 구현하였고, 다음으로는 뉴턴방법을 위한 sin함수등을 구현하였다.

이런식으로 고차 미분 계산을 정리해보면
▪ 고차 미분을 하기 위해 역전파 시 수행되는 계산에 대해서도 연결을 만들도록 함
▪ 역전파의 계산 그래프를 만들수 있음
▪ 고차 미분 외에 어떻게 활용할 수 있는지를 살펴봄
이런식으로 정리 할 수 있다.

